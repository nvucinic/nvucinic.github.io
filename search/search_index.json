{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nvucinic's docs && tips","title":"nvucinic's docs && tips"},{"location":"#nvucinics-docs-tips","text":"","title":"nvucinic's docs &amp;&amp; tips"},{"location":"helm/","text":"Install Helm Install Helm on to your local machine depending on your local OS. To do this, refer to See Helm install steps Initialize Helm on both your server and client with this command: helm init Please make sure your local system is authenticated to use kubectl. This will install tiller on the Kubernetes cluster which is a server side component that stores all your deployment version for easy rollbacks or rollforwards Configure service account for Helm in GKE Run the following commands to setup and configure tiller to use this service account: kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' helm init --service-account tiller --upgrade Tilleres Helm Run Helm and Tiller without install tiller in kubernetes cluster $ export TILLER_NAMESPACE=default $ export HELM_HOST=localhost:44134 $ tiller --storage=secret & $ helm install installdir/","title":"Helm"},{"location":"helm/#install-helm","text":"Install Helm on to your local machine depending on your local OS. To do this, refer to See Helm install steps Initialize Helm on both your server and client with this command: helm init Please make sure your local system is authenticated to use kubectl. This will install tiller on the Kubernetes cluster which is a server side component that stores all your deployment version for easy rollbacks or rollforwards","title":"Install Helm"},{"location":"helm/#configure-service-account-for-helm-in-gke","text":"Run the following commands to setup and configure tiller to use this service account: kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' helm init --service-account tiller --upgrade","title":"Configure service account for Helm in GKE"},{"location":"helm/#tilleres-helm","text":"Run Helm and Tiller without install tiller in kubernetes cluster $ export TILLER_NAMESPACE=default $ export HELM_HOST=localhost:44134 $ tiller --storage=secret & $ helm install installdir/","title":"Tilleres Helm"},{"location":"kops/","text":"Kops on AWS Install Kops MacOS kops install: brew update && brew install kops Alternative install from source $ curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest |grep tag_name | cut -d'\"' -f 4)/kops-darwin-amd64 $ chmod +x ./kops $ sudo mv ./kops/usr/local/bin/ Export AWS env $ export AWS_DEFAULT_REGION=eu-west-1 ## default region $ export AWS_ACCESS_KEY_ID=xxx ## access key $ export AWS_SECRET_ACCESS_KEY=xxx ## secret access key $ export ZONES=$(aws ec2 describe-availability-zones | jq -r '.AvailabilityZones[].ZoneName' | tr '\\n' ',' | tr -d ' ') ## availability zones for nodes $ export NAME=nvkops.k8s.local ## cluster name $ export BUCKET_NAME=nvkops-$(date+%s) ## state bucket name Create iam user and group, add user to group $ aws iam create-group --group-name kops $ aws iam create-user --user-name kops $ aws iam add-user-to-group --user-name kops --group-name kops Create ssh keys $ aws iam create-access-key --user-name kops > kops-creds $ aws ec2 create-key-pair --key-name nvkops | jq -r '.KeyMaterial' > nvkops.pem $ chmod 400 nvkops.pem $ ssh-keygen -y -f nvkops.pem > nvkops.pub Attach group policys for kops group (do not use in production!) aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops --profile devsup-test aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops --profile devsup-test aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops --profile devsup-test aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops --profile devsup-test Create s3 bucket for kops state $ aws s3api create-bucket --bucket $BUCKET_NAME --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION Export kops state stored export KOPS_STATE_STORE=s3://$BUCKET_NAME Create kops cluster with 5 master node and 3 worker nodes kops create cluster --name $NAME --master-count 5 --node-count 3 --node-size t2.small --master-size t2.small --zones $ZONES --master-zones $ZONES --ssh-public-key nvkops.pub --networking kubenet --kubernetes-version v1.16 --yes Command output kops has set your kubectl context to nvkops.k8s.local Cluster is starting. It should be ready in a few minutes. Suggestions: * validate cluster: kops validate cluster * list nodes: kubectl get nodes --show-labels * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.nvkops.k8s.local The admin user is specific to Debian. If not using Debian please use the appropriate user based on your OS. * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs/addons.md Handy kops commands $ kops get cluster $ kops validate cluster","title":"Kops"},{"location":"kops/#kops-on-aws","text":"","title":"Kops on AWS"},{"location":"kops/#install-kops","text":"MacOS kops install: brew update && brew install kops Alternative install from source $ curl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest |grep tag_name | cut -d'\"' -f 4)/kops-darwin-amd64 $ chmod +x ./kops $ sudo mv ./kops/usr/local/bin/","title":"Install Kops"},{"location":"kops/#export-aws-env","text":"$ export AWS_DEFAULT_REGION=eu-west-1 ## default region $ export AWS_ACCESS_KEY_ID=xxx ## access key $ export AWS_SECRET_ACCESS_KEY=xxx ## secret access key $ export ZONES=$(aws ec2 describe-availability-zones | jq -r '.AvailabilityZones[].ZoneName' | tr '\\n' ',' | tr -d ' ') ## availability zones for nodes $ export NAME=nvkops.k8s.local ## cluster name $ export BUCKET_NAME=nvkops-$(date+%s) ## state bucket name","title":"Export AWS env"},{"location":"kops/#create-iam-user-and-group-add-user-to-group","text":"$ aws iam create-group --group-name kops $ aws iam create-user --user-name kops $ aws iam add-user-to-group --user-name kops --group-name kops","title":"Create iam user and group, add user to group"},{"location":"kops/#create-ssh-keys","text":"$ aws iam create-access-key --user-name kops > kops-creds $ aws ec2 create-key-pair --key-name nvkops | jq -r '.KeyMaterial' > nvkops.pem $ chmod 400 nvkops.pem $ ssh-keygen -y -f nvkops.pem > nvkops.pub","title":"Create ssh keys"},{"location":"kops/#attach-group-policys-for-kops-group-do-not-use-in-production","text":"aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops --profile devsup-test aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops --profile devsup-test aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops --profile devsup-test aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops --profile devsup-test","title":"Attach group policys for kops group (do not use in production!)"},{"location":"kops/#create-s3-bucket-for-kops-state","text":"$ aws s3api create-bucket --bucket $BUCKET_NAME --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION","title":"Create s3 bucket for kops state"},{"location":"kops/#export-kops-state-stored","text":"export KOPS_STATE_STORE=s3://$BUCKET_NAME","title":"Export kops state stored"},{"location":"kops/#create-kops-cluster-with-5-master-node-and-3-worker-nodes","text":"kops create cluster --name $NAME --master-count 5 --node-count 3 --node-size t2.small --master-size t2.small --zones $ZONES --master-zones $ZONES --ssh-public-key nvkops.pub --networking kubenet --kubernetes-version v1.16 --yes","title":"Create kops cluster with 5 master node and 3 worker nodes"},{"location":"kops/#command-output","text":"kops has set your kubectl context to nvkops.k8s.local Cluster is starting. It should be ready in a few minutes. Suggestions: * validate cluster: kops validate cluster * list nodes: kubectl get nodes --show-labels * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.nvkops.k8s.local The admin user is specific to Debian. If not using Debian please use the appropriate user based on your OS. * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs/addons.md","title":"Command output"},{"location":"kops/#handy-kops-commands","text":"$ kops get cluster $ kops validate cluster","title":"Handy kops commands"},{"location":"kubernetes/","text":"SSH to node in AKS Run a debian container image and attach a terminal session to it. kubectl run -it --rm aks-ssh --image=debian Once the terminal session is connected to the container, install an SSH client using apt-get apt-get update && apt-get install openssh-client -y Copy your private SSH key into the helper pod. This private key is used to create the SSH into the AKS node. kubectl cp ~/.ssh/id_rsa aks-ssh-554b746bcf-kbwvf:/id_rsa Connect to Azure node ssh -i id_rsa azureuser@10.240.0.0","title":"Kubernetes"},{"location":"kubernetes/#ssh-to-node-in-aks","text":"Run a debian container image and attach a terminal session to it. kubectl run -it --rm aks-ssh --image=debian Once the terminal session is connected to the container, install an SSH client using apt-get apt-get update && apt-get install openssh-client -y Copy your private SSH key into the helper pod. This private key is used to create the SSH into the AKS node. kubectl cp ~/.ssh/id_rsa aks-ssh-554b746bcf-kbwvf:/id_rsa Connect to Azure node ssh -i id_rsa azureuser@10.240.0.0","title":"SSH to node in AKS"},{"location":"prometheus/","text":"Openstack SD config # Scrape OpenStack instances - job_name: 'openstack' openstack_sd_configs: - identity_endpoint: http:// username: vucinic project_name: {{ project_name }} domain_name: Default password: {{ password }} region: {{ region}} role: instance port: 9100 relabel_configs: # Keep only active instances - source_labels: [__meta_openstack_instance_status] action: keep regex: ACTIVE # Keep only instances which are flagged for scraping - source_labels: [__meta_openstack_tag_prometheus_io_scrape] action: keep regex: 'true' # Replace the default instance by the OpenStack instance name - source_labels: [__meta_openstack_instance_name] target_label: instance","title":"Prometheus"},{"location":"prometheus/#openstack-sd-config","text":"# Scrape OpenStack instances - job_name: 'openstack' openstack_sd_configs: - identity_endpoint: http:// username: vucinic project_name: {{ project_name }} domain_name: Default password: {{ password }} region: {{ region}} role: instance port: 9100 relabel_configs: # Keep only active instances - source_labels: [__meta_openstack_instance_status] action: keep regex: ACTIVE # Keep only instances which are flagged for scraping - source_labels: [__meta_openstack_tag_prometheus_io_scrape] action: keep regex: 'true' # Replace the default instance by the OpenStack instance name - source_labels: [__meta_openstack_instance_name] target_label: instance","title":"Openstack SD config"}]}