{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"nvucinic's docs && tips","title":"nvucinic's docs && tips"},{"location":"#nvucinics-docs-tips","text":"","title":"nvucinic's docs &amp;&amp; tips"},{"location":"helm/","text":"Install Helm Install Helm on to your local machine depending on your local OS. To do this, refer to See Helm install steps Initialize Helm on both your server and client with this command: helm init Please make sure your local system is authenticated to use kubectl. This will install tiller on the Kubernetes cluster which is a server side component that stores all your deployment version for easy rollbacks or rollforwards Configure service account for Helm in GKE Run the following commands to setup and configure tiller to use this service account: kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' helm init --service-account tiller --upgrade Tilleres Helm Run Helm and Tiller without install tiller in kubernetes cluster $ export TILLER_NAMESPACE=default $ export HELM_HOST=localhost:44134 $ tiller --storage=secret & $ helm install installdir/","title":"Helm"},{"location":"helm/#install-helm","text":"Install Helm on to your local machine depending on your local OS. To do this, refer to See Helm install steps Initialize Helm on both your server and client with this command: helm init Please make sure your local system is authenticated to use kubectl. This will install tiller on the Kubernetes cluster which is a server side component that stores all your deployment version for easy rollbacks or rollforwards","title":"Install Helm"},{"location":"helm/#configure-service-account-for-helm-in-gke","text":"Run the following commands to setup and configure tiller to use this service account: kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' helm init --service-account tiller --upgrade","title":"Configure service account for Helm in GKE"},{"location":"helm/#tilleres-helm","text":"Run Helm and Tiller without install tiller in kubernetes cluster $ export TILLER_NAMESPACE=default $ export HELM_HOST=localhost:44134 $ tiller --storage=secret & $ helm install installdir/","title":"Tilleres Helm"},{"location":"kubernetes/","text":"SSH to node in AKS Run a debian container image and attach a terminal session to it. kubectl run -it --rm aks-ssh --image=debian Once the terminal session is connected to the container, install an SSH client using apt-get apt-get update && apt-get install openssh-client -y Copy your private SSH key into the helper pod. This private key is used to create the SSH into the AKS node. kubectl cp ~/.ssh/id_rsa aks-ssh-554b746bcf-kbwvf:/id_rsa Connect to Azure node ssh -i id_rsa azureuser@10.240.0.0","title":"Kubernetes"},{"location":"kubernetes/#ssh-to-node-in-aks","text":"Run a debian container image and attach a terminal session to it. kubectl run -it --rm aks-ssh --image=debian Once the terminal session is connected to the container, install an SSH client using apt-get apt-get update && apt-get install openssh-client -y Copy your private SSH key into the helper pod. This private key is used to create the SSH into the AKS node. kubectl cp ~/.ssh/id_rsa aks-ssh-554b746bcf-kbwvf:/id_rsa Connect to Azure node ssh -i id_rsa azureuser@10.240.0.0","title":"SSH to node in AKS"},{"location":"prometheus/","text":"Openstack SD config # Scrape OpenStack instances - job_name: 'openstack' openstack_sd_configs: - identity_endpoint: http:// username: vucinic project_name: {{ project_name }} domain_name: Default password: {{ password }} region: {{ region}} role: instance port: 9100 relabel_configs: # Keep only active instances - source_labels: [__meta_openstack_instance_status] action: keep regex: ACTIVE # Keep only instances which are flagged for scraping - source_labels: [__meta_openstack_tag_prometheus_io_scrape] action: keep regex: 'true' # Replace the default instance by the OpenStack instance name - source_labels: [__meta_openstack_instance_name] target_label: instance","title":"Prometheus"},{"location":"prometheus/#openstack-sd-config","text":"# Scrape OpenStack instances - job_name: 'openstack' openstack_sd_configs: - identity_endpoint: http:// username: vucinic project_name: {{ project_name }} domain_name: Default password: {{ password }} region: {{ region}} role: instance port: 9100 relabel_configs: # Keep only active instances - source_labels: [__meta_openstack_instance_status] action: keep regex: ACTIVE # Keep only instances which are flagged for scraping - source_labels: [__meta_openstack_tag_prometheus_io_scrape] action: keep regex: 'true' # Replace the default instance by the OpenStack instance name - source_labels: [__meta_openstack_instance_name] target_label: instance","title":"Openstack SD config"}]}